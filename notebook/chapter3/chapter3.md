# 3.1 Linear Basis Function Models
## 线性基函数模型

### 线性回归的最简单形式
最简单的线性回归模型是输入变量的线性组合：

$$
y(x, w) = w_0 + w_1 x_1 + \dots + w_D x_D \tag{3.1}
$$

其中，$x = (x_1, \dots, x_D)^T$。  
这一模型通常被称为**线性回归**。其关键性质是：  
- 它是参数 $w_0, \dots, w_D$ 的**线性函数**；  
- 同时也是输入变量 $x_i$ 的**线性函数**，这对模型施加了显著的限制。  

---

### 引入基函数
为扩展模型的表达能力，我们考虑输入变量的固定非线性函数的线性组合：

$$
y(x, w) = w_0 + \sum_{j=1}^{M-1} w_j \phi_j(x) \tag{3.2}
$$

其中，$\phi_j(x)$ 被称为**基函数**（basis functions）。  
模型参数的总数为 $M$。  

- $w_0$ 用于表示数据的固定偏移量（称为**偏置参数**，不要与统计学意义上的 bias 混淆）。  
- 定义一个虚拟基函数 $\phi_0(x) = 1$，则可将模型写为：  

$$
y(x, w) = \sum_{j=0}^{M-1} w_j \phi_j(x) = w^T \phi(x) \tag{3.3}
$$

其中：  
- $w = (w_0, \dots, w_{M-1})^T$  
- $\phi(x) = (\phi_0(x), \dots, \phi_{M-1}(x))^T$  

这样，模型形式在符号上更加简洁。

---

### 非线性基函数
通过使用非线性基函数，$y(x, w)$ 可以成为 $x$ 的非线性函数。  
这种模型仍称为“线性模型”，因为它在参数 $w$ 上是线性的。  
这一点大大简化了分析，但也带来一些局限性（详见 3.6 节）。  

#### 1. 多项式基函数
在 **第 1 章的多项式回归**中，若只有一个输入变量 $x$，则基函数为：

$$
\phi_j(x) = x^j
$$

但多项式基函数是**全局函数**，即输入空间某一区域的变化会影响所有区域。  
一种解决方法是将输入空间划分为多个区域，在每个区域拟合一个多项式，从而得到 **样条函数 (splines)**。  

#### 2. 高斯基函数
另一种常用的基函数是高斯函数：

$$
\phi_j(x) = \exp \Bigg( - \frac{(x - \mu_j)^2}{2s^2} \Bigg) \tag{3.4}
$$

- $\mu_j$ 控制基函数在输入空间的位置；  
- $s$ 控制空间尺度（宽度）。  

这些函数通常被称为**高斯基函数**，但它们不必具有概率意义，也不需要归一化，因为它们会被参数 $w_j$ 缩放。  

#### 3. Sigmoid 基函数
另一类选择是 Sigmoid 基函数：

$$
\phi_j(x) = \sigma\Bigg(\frac{x - \mu_j}{s}\Bigg) \tag{3.5}
$$

其中 logistic sigmoid 定义为：  

$$
\sigma(a) = \frac{1}{1 + \exp(-a)} \tag{3.6}
$$

等价地，可以使用 **tanh** 函数：  

$$
\tanh(a) = 2\sigma(a) - 1
$$

因此 logistic sigmoid 与 tanh 的线性组合在本质上是等价的。  

#### 4. Fourier 基函数
另一种可能选择是 **傅里叶基函数**，它们展开为正弦函数与余弦函数。  
- 每个基函数对应一个特定频率，空间上是无限的。  
- 与之对比，局部化的基函数（如高斯）会涉及一系列不同频率。  

#### 5. 小波基函数
在信号处理中，常常需要同时局部化于**空间**和**频率**的基函数，形成 **小波（wavelets）**。  
- 小波函数通常定义为**正交**的，便于使用。  
- 小波特别适合输入是规则格点的情况，例如时间序列或图像像素。  

---

### 小结
- 不同基函数的选择导致了不同的模型表现。  
- 多数讨论与基函数具体形式无关，因此除数值举例外，我们通常不指定具体的 $\phi_j(x)$。  
- 特殊情况：如果 $\phi(x) = x$，那么模型退化为最简单的线性回归。  
- 本章主要讨论单一目标变量 $t$，但在 **3.1.5 节** 中会简要介绍多目标情形的修正方法。  


## 3.1.1 Maximum likelihood and least squares

### 高斯噪声假设
假设目标变量 $t$ 由确定性函数 $y(x, w)$ 与加性高斯噪声组成：

$$
t = y(x, w) + \epsilon \tag{3.7}
$$

其中，$\epsilon$ 是零均值、精度（方差的倒数）为 $\beta$ 的高斯噪声。  
因此，条件分布为：

$$
p(t|x, w, \beta) = \mathcal{N}(t \mid y(x, w), \beta^{-1}) \tag{3.8}
$$

由于假设为高斯分布，条件均值就是预测值：

$$
\mathbb{E}[t|x] = \int t \, p(t|x) dt = y(x, w) \tag{3.9}
$$

⚠️ 注意：高斯噪声假设意味着条件分布是单峰的，这在某些应用中可能不合适。多峰情况可用 **混合高斯条件分布** 扩展（见 14.5.1）。

---

### 数据集与似然函数
考虑数据集 $X = \{x_1, \dots, x_N\}$，对应目标值 $t_1, \dots, t_N$，将目标值整理成向量：

$$
\mathbf{t} = (t_1, \dots, t_N)^T
$$

假设各数据点独立，则似然函数为：

$$
p(\mathbf{t}|X, w, \beta) = \prod_{n=1}^N \mathcal{N}(t_n \mid w^T \phi(x_n), \beta^{-1}) \tag{3.10}
$$

取对数得到：

$$
\ln p(\mathbf{t}|w, \beta) 
= \frac{N}{2}\ln \beta - \frac{N}{2}\ln (2\pi) - \beta E_D(w) \tag{3.11}
$$

其中误差函数为：

$$
E_D(w) = \frac{1}{2} \sum_{n=1}^N \{t_n - w^T \phi(x_n)\}^2 \tag{3.12}
$$

<span stype="color: red">3.10到3.11，带入正态分布的公式就很容易得到，3.12得出的误差函数的形式是不是可以解释为什么线性回归都是最小化这个东西（3.12）</span>

---

### 最大似然与最小二乘的关系
最大化对数似然等价于最小化平方误差 $E_D(w)$。  
其梯度为：

$$
\nabla \ln p(\mathbf{t}|w, \beta) 
= \sum_{n=1}^N (t_n - w^T \phi(x_n)) \, \phi(x_n)^T \tag{3.13}
$$

令其为零，得到：

$$
0 = \sum_{n=1}^N t_n \phi(x_n)^T - w^T \Bigg( \sum_{n=1}^N \phi(x_n)\phi(x_n)^T \Bigg) \tag{3.14}
$$

解得最大似然估计：

$$
w_{\text{ML}} = (\Phi^T \Phi)^{-1} \Phi^T \mathbf{t} \tag{3.15}
$$

这就是 **最小二乘法的正规方程 (normal equations)**。  
其中设计矩阵 $\Phi$ 定义为：

$$
\Phi = 
\begin{pmatrix}
\phi_0(x_1) & \phi_1(x_1) & \cdots & \phi_{M-1}(x_1) \\
\phi_0(x_2) & \phi_1(x_2) & \cdots & \phi_{M-1}(x_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(x_N) & \phi_1(x_N) & \cdots & \phi_{M-1}(x_N)
\end{pmatrix} \tag{3.16}
$$

其 **Moore–Penrose 伪逆** 定义为：

$$
\Phi^\dagger \equiv (\Phi^T \Phi)^{-1} \Phi^T \tag{3.17}
$$

若 $\Phi$ 可逆，则退化为 $\Phi^{-1}$。

---

### 偏置参数的作用
若显式写出偏置 $w_0$，误差函数为：

$$
E_D(w) = \frac{1}{2} \sum_{n=1}^N \Big\{t_n - w_0 - \sum_{j=1}^{M-1} w_j \phi_j(x_n)\Big\}^2 \tag{3.18}
$$

对 $w_0$ 求导并令其为零，得：

$$
w_0 = \bar{t} - \sum_{j=1}^{M-1} w_j \bar{\phi}_j \tag{3.19}
$$

其中：

$$
\bar{t} = \frac{1}{N}\sum_{n=1}^N t_n, \quad
\bar{\phi}_j = \frac{1}{N}\sum_{n=1}^N \phi_j(x_n) \tag{3.20}
$$

即：**偏置参数 $w_0$ 用于补偿目标值平均数与基函数平均加权和之间的差异。**

---

### 噪声精度的最大似然估计
对 (3.11) 关于 $\beta$ 最大化，得到：

$$
\frac{1}{\beta_{\text{ML}}} 
= \frac{1}{N} \sum_{n=1}^N \{t_n - w_{\text{ML}}^T \phi(x_n)\}^2 \tag{3.21}
$$

即：噪声方差的估计值等于残差平方的均值。

---

### 几何解释
- 在 $N$ 维空间中，坐标轴对应目标值 $t_1, \dots, t_N$；  
- 每个基函数 $\phi_j(x)$ 可视作长度为 $N$ 的向量 $\varphi_j$；  
- 最小二乘解就是数据向量 $\mathbf{t}$ 在由 $\{\varphi_j\}$ 张成的子空间上的**正交投影**。  
